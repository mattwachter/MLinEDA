{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F0LBgGMcfgxc"
   },
   "source": [
    "This Python Jupyter Notebook can be run on colab.research.google.com. \n",
    "Enable GPU in menu Runtime --> Change Runtime Type.\n",
    "The rpml folder with Data/Training_Data_and_Labels and (empty) Data/NNResults subfolders from this repo needs to be uploaded to a Google Drive to enable access for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "suYHyeeWo5_B",
    "outputId": "bc050843-e8b6-4cc2-cc1b-7575a4acd009"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jy3EYUqYeX6H"
   },
   "source": [
    "Try to access the `rpml` folder with `Data/Training_Data_and_Labels` and (empty) `Data/NNResults` subfolders on your Google Drive and change `root_dir` to the path of `rpml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0jel0pwqrbwv",
    "outputId": "26cb7d9f-9f1f-452b-85cd-615aef239078"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\n"
     ]
    }
   ],
   "source": [
    "!ls /content/drive/My\\ Drive/rpml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "pywqwCRSpeCT",
    "outputId": "d358edea-73cf-44f5-9c21-f124b40bc44a"
   },
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Runs neural network on data in folder 'Data/Training_Data_and_labels'\n",
    "\n",
    "Adapted from 'main.py' on 2019-08-14 by Robert Fischbach.\n",
    "Uses parameters guessed manually, without using Talos.\n",
    "\"\"\"\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "import tensorflow as tf\n",
    "print(\"GPU Available: \", tf.test.is_gpu_available())\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "\n",
    "# ************************* Config *************************************\n",
    "# Change this to the directory of the local git repository.\n",
    "root_dir = '/content/drive/My Drive/rpml/'\n",
    "# Control numerical precision, can be 'float16', 'float32', or 'float64'\n",
    "floatx_wanted = 'float32'\n",
    "K.set_floatx(floatx_wanted)\n",
    "# Choose wether to normalize data to the mean and standard deviation of the\n",
    "# training data. Not clear if it improves or hampers performance.\n",
    "normalize_data = True\n",
    "# Use a unique name for each experiment so that the results are saved separately\n",
    "experiment_name = '1500epochs'\n",
    "# ************************* Config *************************************\n",
    "\n",
    "\n",
    "def short_model(x_train, y_train, x_val, y_val):\n",
    "    \"\"\"Run a Keras sequential Neural Network.\n",
    "\n",
    "    Args:\n",
    "        x_train (numpy array):                  Training features\n",
    "        y_train (numpy array):                  Training labels\n",
    "        x_val (numpy array):                    Validation features\n",
    "        y_val (numpy array):                    Validation labels\n",
    "        metrics_cb (inst of Keras Callback()):  Callback metric\n",
    "\n",
    "    Returns:\n",
    "        history [keras.callbacks.History]: Data about training history\n",
    "        model   [keras.models.Sequential]: Fitted Neural Network\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_shape=(x_train.shape[1],), activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='Nadam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['acc', 'binary_crossentropy'])\n",
    "\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        batch_size= 20000,   #int(y_train.shape[0] * 0.1),\n",
    "                        epochs=1500,\n",
    "                        validation_data=[x_val, y_val],\n",
    "                        verbose=1,\n",
    "                        class_weight={0:1.0, 1:25.0}\n",
    "                        )\n",
    "    return history, model\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_bools(nparray):\n",
    "    \"\"\"Replaces all above-one values in a list of integer values with '1'\n",
    "\n",
    "    Args:\n",
    "        nparray:   Numpy array\n",
    "\n",
    "    Returns:\n",
    "        binary_nparray:    Numpy array consisting of '0's and '1's\n",
    "    \"\"\"\n",
    "    ones = np.ones(np.shape(nparray))\n",
    "    binary_nparray = np.minimum(nparray, ones)\n",
    "    return binary_nparray\n",
    "\n",
    "\n",
    "def main(root_dir):\n",
    "    \"\"\"Run neural networks with imported feature and label data and save them.\n",
    "\n",
    "    Args:\n",
    "        root_dir (string):      directory with the Data/NNResults and\n",
    "                                Data/Training_Data_and_Labels folders\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Leave out  mgc_fft_2, like Tabrizi 2018, to have a 'clean' test case.\n",
    "    benchmark_names = ['mgc_des_perf_1', 'mgc_des_perf_a', 'mgc_des_perf_b',\n",
    "                    'mgc_fft_1', 'mgc_fft_a',\n",
    "                    'mgc_matrix_mult_1', 'mgc_matrix_mult_a',\n",
    "                    'mgc_pci_bridge32_a', 'mgc_pci_bridge32_b']\n",
    "    features = np.zeros((0, 9, 8))\n",
    "    labels = np.zeros((0,))\n",
    "    for i in range(len(benchmark_names)):\n",
    "        benchmark_name = benchmark_names[i]\n",
    "        folder_prefix_training = root_dir + 'Data/Training_Data_and_Labels/design_placed_training_data_'\n",
    "        folder_prefix_labels = root_dir + 'Data/Training_Data_and_Labels/design_routed_labels_shorts_'\n",
    "\n",
    "        train_to_validation_ratio = 0.8\n",
    "        assert(0 < train_to_validation_ratio < 1)\n",
    "\n",
    "        fh = open(folder_prefix_training + benchmark_name + '.pickle', 'br')\n",
    "        features_cur = pickle.load(fh)\n",
    "        fh.close()\n",
    "\n",
    "        fh = open(folder_prefix_labels + benchmark_name + '.pickle', 'br')\n",
    "        labels_cur = pickle.load(fh)\n",
    "        fh.close()\n",
    "\n",
    "        labels_cur = np.asarray(labels_cur)\n",
    "        features_cur = np.asarray(features_cur)\n",
    "\n",
    "        # Convert numbers of shorts to binary 0s and 1s (exist or not).\n",
    "        labels_cur = convert_to_bools(labels_cur)\n",
    "        print(labels_cur)\n",
    "        labels_cur = np.asarray(labels_cur)\n",
    "\n",
    "        print('labels.shape', labels.shape)\n",
    "        print('labels_cur.shape', labels_cur.shape)\n",
    "        print('features_cur.shape', features_cur.shape)\n",
    "        print('features.shape', features.shape)\n",
    "        labels = np.concatenate((labels, labels_cur), axis=0)\n",
    "        features = np.concatenate((features, features_cur), axis=0)\n",
    "    assert(len(features) == len(labels))\n",
    "    print('labels.shape:', labels.shape)\n",
    "    print('features.shape:', features.shape)\n",
    "    benchmark_name = 'combined_without_mgc_fft_2'\n",
    "\n",
    "    print('\\nFitting model for benchmark', benchmark_name)\n",
    "\n",
    "    x, y = shuffle(features, labels, random_state=666)\n",
    "    x = np.array(x)\n",
    "\n",
    "    x = x.reshape(x.shape[0], x.shape[1] * x.shape[2])\n",
    "    y = np.array(y)\n",
    "    print('Total number of shorts:', np.sum(y))\n",
    "    print('x.shape:', x.shape)\n",
    "    print('y.shape:', y.shape)\n",
    "    \n",
    "    num_training_samples = int(len(x) * train_to_validation_ratio)\n",
    "    x_train = x[:num_training_samples]\n",
    "    y_train = y[:num_training_samples]\n",
    "    x_val = x[num_training_samples:]\n",
    "    y_val = y[num_training_samples:]\n",
    "\n",
    "    if normalize_data:\n",
    "        # Normalize Data as in 'Deep Learning with Python', p.86\n",
    "        mean = x_train.mean(axis=0)\n",
    "        x_train -= mean\n",
    "        std = x_train.std(axis=0)\n",
    "        x_train /= std\n",
    "        # Normalize x_val with mean and std from x_train to prevent data leakage\n",
    "        x_val -= mean\n",
    "        x_val /= std\n",
    "    \n",
    "    history, model = short_model(x_train, y_train, x_val, y_val)\n",
    "\n",
    "    # Save history dictionary and model\n",
    "    fname_history = root_dir + 'Data/NNResults/' + benchmark_name + '_' + \\\n",
    "                    experiment_name + '_history.pickle'\n",
    "    with open(fname_history, 'wb') as f:\n",
    "        pickle.dump(history.history, f, pickle.DEFAULT_PROTOCOL)\n",
    "\n",
    "    fname_model = root_dir + 'Data/NNResults/' + benchmark_name + '_' + \\\n",
    "                  experiment_name + '_model.h5' \n",
    "    model.save(fname_model)\n",
    "    \n",
    "\n",
    "# Do not execute main() when imported as module\n",
    "if __name__ == '__main__':\n",
    "    main(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "colab_type": "code",
    "id": "_wrks3v4VWK-",
    "outputId": "6c82128c-429d-44fa-dd40-a2e775710bb0"
   },
 "source": [
    "!ls -a /content/drive/My\\ Drive/rpml/Data/NNResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HA3_-7V01nVs"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NeuralNetwork_GoogleColab_GPU.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
