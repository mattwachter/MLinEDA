{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F0LBgGMcfgxc"
   },
   "source": [
    "This Python Jupyter Notebook can be run on colab.research.google.com. According to this [Github Issue(https://github.com/tensorflow/tensorflow/issues/32043)] it should work once Tensorflow 2.1 is available on Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xYkzi3XetSL-",
    "outputId": "64ff040e-eb79-40dd-c699-c38b561bfb1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 2.x selected.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 2.x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "suYHyeeWo5_B",
    "outputId": "f99e045d-d872-42d7-b880-50c7d880dbe7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jy3EYUqYeX6H"
   },
   "source": [
    "Try to access the `rpml` folder with `Data/Training_Data_and_Labels` and (empty) `Data/NNResults` subfolders on your Google Drive and change `root_dir` to the path of `rpml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0jel0pwqrbwv",
    "outputId": "0cae0a8f-76b4-4f6f-c221-79de75387b96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/content/drive/My Drive/rpml1/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls /content/drive/My\\ Drive/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "pywqwCRSpeCT",
    "outputId": "705b8762-4793-4d3d-ddaa-61e870557c43"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Runs neural network on data in folder 'Data/Training_Data_and_labels'\n",
    "\n",
    "Adapted from 'main.py' on 2019-08-14 by Robert Fischbach.\n",
    "Uses parameters guessed manually, without using Talos.\n",
    "\"\"\"\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "\n",
    "# ************************* Config *************************************\n",
    "# Change this to the directory of the local git repository.\n",
    "root_dir = '/content/drive/My Drive/rpml/'\n",
    "# Control numerical precision, can be 'float16', 'float32', or 'float64'\n",
    "floatx_wanted = 'float32'\n",
    "K.set_floatx(floatx_wanted)\n",
    "# Choose wether to normalize data to the mean and standard deviation of the\n",
    "# training data. Not clear if it improves or hampers performance.\n",
    "normalize_data = True\n",
    "# Use a unique name for each experiment so that the results are saved separately\n",
    "experiment_name = '1500epochs'\n",
    "# ************************* Config *************************************\n",
    "\n",
    "\n",
    "def train_input_fn(batch_size=1024):\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_train,y_train))\n",
    "# Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.shuffle(1000, reshuffle_each_iteration=True)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "# Return the dataset.\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def short_model(x_train, y_train, x_val, y_val):\n",
    "    \"\"\"Run a Keras sequential Neural Network.\n",
    "\n",
    "    Args:\n",
    "        x_train (numpy array):                  Training features\n",
    "        y_train (numpy array):                  Training labels\n",
    "        x_val (numpy array):                    Validation features\n",
    "        y_val (numpy array):                    Validation labels\n",
    "        metrics_cb (inst of Keras Callback()):  Callback metric\n",
    "\n",
    "    Returns:\n",
    "        history [keras.callbacks.History]: Data about training history\n",
    "        model   [keras.models.Sequential]: Fitted Neural Network\n",
    "    \"\"\"\n",
    "    \n",
    "    cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_host(cluster_resolver.master())\n",
    "    tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
    "    tpu_strategy = tf.distribute.experimental.TPUStrategy(cluster_resolver)\n",
    "    \n",
    "# The commented out version works with Tensorflow 1.x, but does not support \n",
    "# `class_weight`\n",
    "#     resolver = tf.contrib.cluster_resolver.TPUClusterResolver('grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
    "#     tf.contrib.distribute.initialize_tpu_system(resolver)\n",
    "#     strategy = tf.contrib.distribute.TPUStrategy(resolver)\n",
    "#   with strategy.scope():    \n",
    "    with tpu_strategy.scope():\n",
    "      model = Sequential()\n",
    "      model.add(Dense(16, input_shape=(x_train.shape[1],), activation='relu'))\n",
    "      model.add(Dense(16, activation='relu'))\n",
    "      model.add(Dense(1, activation='sigmoid'))\n",
    "      model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.25),\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['acc', 'binary_crossentropy'])\n",
    "\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        batch_size= 20000,   #int(y_train.shape[0] * 0.1),\n",
    "                        epochs=1500,\n",
    "                        validation_data=[x_val, y_val],\n",
    "                        verbose=1,\n",
    "                        class_weight={0:1.0, 1:25.0}\n",
    "                        )\n",
    "    return history, model\n",
    "\n",
    "\n",
    "def convert_to_bools(nparray):\n",
    "    \"\"\"Replaces all above-one values in a list of integer values with '1'\n",
    "\n",
    "    Args:\n",
    "        nparray:   Numpy array\n",
    "\n",
    "    Returns:\n",
    "        binary_nparray:    Numpy array consisting of '0's and '1's\n",
    "    \"\"\"\n",
    "    ones = np.ones(np.shape(nparray))\n",
    "    binary_nparray = np.minimum(nparray, ones)\n",
    "    return binary_nparray\n",
    "\n",
    "\n",
    "def main(root_dir):\n",
    "    \"\"\"Run neural networks with imported feature and label data and save them.\n",
    "\n",
    "    Args:\n",
    "        root_dir (string):      directory with the Data/NNResults and\n",
    "                                Data/Training_Data_and_Labels folders\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Leave out  mgc_fft_2, like Tabrizi 2018, to have a 'clean' test case.\n",
    "    benchmark_names = ['mgc_des_perf_1', 'mgc_des_perf_a', 'mgc_des_perf_b',\n",
    "                    'mgc_fft_1', 'mgc_fft_a',\n",
    "                    'mgc_matrix_mult_1', 'mgc_matrix_mult_a',\n",
    "                    'mgc_pci_bridge32_a', 'mgc_pci_bridge32_b']\n",
    "    features = np.zeros((0, 9, 8))\n",
    "    labels = np.zeros((0,))\n",
    "    for i in range(len(benchmark_names)):\n",
    "        benchmark_name = benchmark_names[i]\n",
    "        folder_prefix_training = root_dir + 'Data/Training_Data_and_Labels/design_placed_training_data_'\n",
    "        folder_prefix_labels = root_dir + 'Data/Training_Data_and_Labels/design_routed_labels_shorts_'\n",
    "\n",
    "        train_to_validation_ratio = 0.8\n",
    "        assert(0 < train_to_validation_ratio < 1)\n",
    "\n",
    "        fh = open(folder_prefix_training + benchmark_name + '.pickle', 'br')\n",
    "        features_cur = pickle.load(fh)\n",
    "        fh.close()\n",
    "\n",
    "        fh = open(folder_prefix_labels + benchmark_name + '.pickle', 'br')\n",
    "        labels_cur = pickle.load(fh)\n",
    "        fh.close()\n",
    "\n",
    "        labels_cur = np.asarray(labels_cur)\n",
    "        features_cur = np.asarray(features_cur)\n",
    "\n",
    "        # Convert numbers of shorts to binary 0s and 1s (exist or not).\n",
    "        labels_cur = convert_to_bools(labels_cur)\n",
    "        print(labels_cur)\n",
    "        labels_cur = np.asarray(labels_cur)\n",
    "\n",
    "        print('labels.shape', labels.shape)\n",
    "        print('labels_cur.shape', labels_cur.shape)\n",
    "        print('features_cur.shape', features_cur.shape)\n",
    "        print('features.shape', features.shape)\n",
    "        labels = np.concatenate((labels, labels_cur), axis=0)\n",
    "        features = np.concatenate((features, features_cur), axis=0)\n",
    "    assert(len(features) == len(labels))\n",
    "    print('labels.shape:', labels.shape)\n",
    "    print('features.shape:', features.shape)\n",
    "    benchmark_name = 'combined_without_mgc_fft_2'\n",
    "\n",
    "    print('\\nFitting model for benchmark', benchmark_name)\n",
    "\n",
    "    x, y = shuffle(features, labels, random_state=666)\n",
    "    x = np.array(x)\n",
    "\n",
    "    x = x.reshape(x.shape[0], x.shape[1] * x.shape[2])\n",
    "    y = np.array(y)\n",
    "    print('Total number of shorts:', np.sum(y))\n",
    "    print('x.shape:', x.shape)\n",
    "    print('y.shape:', y.shape)\n",
    "    \n",
    "    num_training_samples = int(len(x) * train_to_validation_ratio)\n",
    "    x_train = x[:num_training_samples]\n",
    "    y_train = y[:num_training_samples]\n",
    "    x_val = x[num_training_samples:]\n",
    "    y_val = y[num_training_samples:]\n",
    "\n",
    "    if normalize_data:\n",
    "        # Normalize Data as in 'Deep Learning with Python', p.86\n",
    "        mean = x_train.mean(axis=0)\n",
    "        x_train -= mean\n",
    "        std = x_train.std(axis=0)\n",
    "        x_train /= std\n",
    "        # Normalize x_val with mean and std from x_train to prevent data leakage\n",
    "        x_val -= mean\n",
    "        x_val /= std\n",
    "    \n",
    "    history, model = short_model(x_train, y_train, x_val, y_val)\n",
    "\n",
    "    # Save history dictionary and model\n",
    "    fname_history = root_dir + 'Data/NNResults/' + benchmark_name + '_' + \\\n",
    "                    experiment_name + '_history.pickle'\n",
    "    with open(fname_history, 'wb') as f:\n",
    "        pickle.dump(history.history, f, pickle.DEFAULT_PROTOCOL)\n",
    "\n",
    "    fname_model = root_dir + 'Data/NNResults/' + benchmark_name + '_' + \\\n",
    "                  experiment_name + '_model.h5' \n",
    "    model.save(fname_model)\n",
    "    \n",
    "\n",
    "# Do not execute main() when imported as module\n",
    "if __name__ == '__main__':\n",
    "    main(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_wrks3v4VWK-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NeuralNetwork_GoogleColab_TPU.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
